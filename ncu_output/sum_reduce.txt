 kernel_cutlass_reduce_sum_kernel_last_tensorptrf32gmemo4096409640961_tensorptrf32gmemo40961_4_0 (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       100174
    Memory Throughput                 %        93.81
    DRAM Throughput                   %        93.81
    Duration                         us        94.11
    L1/TEX Cache Throughput           %        15.18
    L2 Cache Throughput               %        33.00
    SM Active Cycles              cycle     97308.30
    Compute (SM) Throughput           %         9.62
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.
          Start by analyzing DRAM in the Memory Workload Analysis section.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved
          close to 1% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details
          on roofline analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        50.33
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.33
    Executed Ipc Elapsed  inst/cycle         0.32
    Issue Slots Busy               %         8.18
    Issued Ipc Active     inst/cycle         0.33
    SM Busy                        %         8.18
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.37%
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       809.32
    Mem Busy                               %        17.07
    Max Bandwidth                          %        93.81
    L1/TEX Hit Rate                        %         0.02
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %         0.58
    Mem Pipes Busy                         %         9.62
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 25.74%
          The memory access pattern for global stores to L2 might not be optimal. On average, only 4.0 of the 32 bytes
          transmitted per sector are utilized by each thread. This applies to the 89.2% of sectors missed in L1TEX.
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced
          global stores.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.20
    Issued Warp Per Scheduler                        0.08
    No Eligible                            %        91.80
    Active Warps Per Scheduler          warp         9.36
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.188%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 12.2 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average
          of 9.36 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       114.14
    Warp Cycles Per Executed Instruction           cycle       114.69
    Avg. Active Threads Per Warp                                31.89
    Avg. Not Predicated Off Threads Per Warp                    31.29
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.188%
          On average, each warp of this workload spends 106.6 cycles being stalled waiting for a scoreboard dependency
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently
          used data to shared memory. This stall type represents about 93.4% of the total average of 114.1 cycles
          between issuing two instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on
          sampling data. The Kernel Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details
          on each stall reason.

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      7917.97
    Executed Instructions                           inst      4497408
    Avg. Issued Instructions Per Scheduler          inst      7955.98
    Issued Instructions                             inst      4518996
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.056%
          This kernel executes 0 fused and 675840 non-fused FP32 instructions. By converting pairs of non-fused
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              42
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             128
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             142
    Stack Size                                                  1024
    Threads                                   thread          524288
    # TPCs                                                        71
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.88
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 1257 thread
          blocks. Under the assumption of a uniform execution duration of all thread blocks, this partial wave may
          account for up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The
          overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        77.93
    Achieved Active Warps Per SM           warp        37.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    793401.33
    Total DRAM Elapsed Cycles        cycle     10148864
    Average L1 Active Cycles         cycle     97308.30
    Total L1 Elapsed Cycles          cycle     14224546
    Average L2 Active Cycles         cycle    127385.48
    Total L2 Elapsed Cycles          cycle      6356352
    Average SM Active Cycles         cycle     97308.30
    Total SM Elapsed Cycles          cycle     14224546
    Average SMSP Active Cycles       cycle     97001.06
    Total SMSP Elapsed Cycles        cycle     56898184
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.02
    Branch Instructions              inst       106496
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  kernel_cutlass_reduce_sum_kernel_last_tensorptrf32gmemo4096409640961_tensorptrf32gmemo40961_4_0 (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         8.99
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       100250
    Memory Throughput                 %        93.71
    DRAM Throughput                   %        93.71
    Duration                         us        94.18
    L1/TEX Cache Throughput           %        15.20
    L2 Cache Throughput               %        33.26
    SM Active Cycles              cycle     97148.04
    Compute (SM) Throughput           %         9.61
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.
          Start by analyzing DRAM in the Memory Workload Analysis section.

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved
          close to 1% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details
          on roofline analysis.

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        50.33
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.33
    Executed Ipc Elapsed  inst/cycle         0.32
    Issue Slots Busy               %         8.19
    Issued Ipc Active     inst/cycle         0.33
    SM Busy                        %         8.19
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.36%
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       808.46
    Mem Busy                               %        17.15
    Max Bandwidth                          %        93.71
    L1/TEX Hit Rate                        %         0.02
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %         0.58
    Mem Pipes Busy                         %         9.61
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 25.87%
          The memory access pattern for global stores to L2 might not be optimal. On average, only 4.0 of the 32 bytes
          transmitted per sector are utilized by each thread. This applies to the 88.9% of sectors missed in L1TEX.
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced
          global stores.

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.20
    Issued Warp Per Scheduler                        0.08
    No Eligible                            %        91.80
    Active Warps Per Scheduler          warp         9.36
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.29%
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only
          issues an instruction every 12.2 cycles. This might leave hardware resources underutilized and may lead to
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average
          of 9.36 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       114.07
    Warp Cycles Per Executed Instruction           cycle       114.62
    Avg. Active Threads Per Warp                                31.89
    Avg. Not Predicated Off Threads Per Warp                    31.29
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.29%
          On average, each warp of this workload spends 106.6 cycles being stalled waiting for a scoreboard dependency
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently
          used data to shared memory. This stall type represents about 93.5% of the total average of 114.1 cycles
          between issuing two instructions.
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on
          sampling data. The Kernel Profiling Guide
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details
          on each stall reason.

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      7917.97
    Executed Instructions                           inst      4497408
    Avg. Issued Instructions Per Scheduler          inst      7956.46
    Issued Instructions                             inst      4519271
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.06%
          This kernel executes 0 fused and 675840 non-fused FP32 instructions. By converting pairs of non-fused
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              42
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             128
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             142
    Stack Size                                                  1024
    Threads                                   thread          524288
    # TPCs                                                        71
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.88
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 1257 thread
          blocks. Under the assumption of a uniform execution duration of all thread blocks, this partial wave may
          account for up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The
          overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        78.04
    Achieved Active Warps Per SM           warp        37.46
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    793101.33
    Total DRAM Elapsed Cycles        cycle     10156032
    Average L1 Active Cycles         cycle     97148.04
    Total L1 Elapsed Cycles          cycle     14235170
    Average L2 Active Cycles         cycle    126206.75
    Total L2 Elapsed Cycles          cycle      6306576
    Average SM Active Cycles         cycle     97148.04
    Total SM Elapsed Cycles          cycle     14235170
    Average SMSP Active Cycles       cycle     97011.88
    Total SMSP Elapsed Cycles        cycle     56940680
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.02
    Branch Instructions              inst       106496
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------
